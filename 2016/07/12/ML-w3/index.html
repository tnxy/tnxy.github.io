
 <!DOCTYPE HTML>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
  
    <title>ML-w3 | xiyuのworkspace</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="xiyu">
    
    <meta name="description" content="机器学习课程笔记系列 I —— Andrew Ng w3

监督学习-classification问题##Example

Email:Spame / not spam?
Online Transactions:Fraudulent (yes/no)?
Tumor: Malignant/Benign">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="xiyuのworkspace" title="xiyuのworkspace"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="xiyuのworkspace">xiyuのworkspace</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:tnxy.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/07/12/ML-w3/" title="ML-w3" itemprop="url">ML-w3</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://tnxy.github.io" title="xiyu">xiyu</a>
    </p>
  <p class="article-time">
    <time datetime="2016-07-12T14:03:47.000Z" itemprop="datePublished">2016-07-12</time>
    Updated:<time datetime="2016-07-12T14:14:25.000Z" itemprop="dateModified">2016-07-12</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#监督学习-classification问题"><span class="toc-number">1.</span> <span class="toc-text">监督学习-classification问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Preface"><span class="toc-number">1.1.</span> <span class="toc-text">Preface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-Model"><span class="toc-number">1.2.</span> <span class="toc-text">Logistic Regression Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition"><span class="toc-number">1.2.1.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cost-function"><span class="toc-number">1.2.2.</span> <span class="toc-text">cost function</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#common-cost-function"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">common cost function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cost-function-for-logistic-regression"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">cost function for logistic regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#证明cost-function"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">证明cost function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#minimize-J"><span class="toc-number">1.2.3.</span> <span class="toc-text">minimize J</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Gradient-descent-algorithm"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">1.Gradient descent algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-other-alogrithms"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">2.other alogrithms</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pseudocode"><span class="toc-number">1.2.4.</span> <span class="toc-text">pseudocode</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multiclass-Classification"><span class="toc-number">1.3.</span> <span class="toc-text">Multiclass Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Example"><span class="toc-number">1.3.1.</span> <span class="toc-text">Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solution"><span class="toc-number">1.3.2.</span> <span class="toc-text">Solution</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Solving-overfitting-problem"><span class="toc-number">2.</span> <span class="toc-text">Solving overfitting problem</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition-overfitting"><span class="toc-number">2.1.</span> <span class="toc-text">Definition overfitting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Solution-Regularation"><span class="toc-number">2.2.</span> <span class="toc-text">Solution - Regularation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-cost-function"><span class="toc-number">2.2.1.</span> <span class="toc-text">Model-cost function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularized-Linear-Regression"><span class="toc-number">2.2.2.</span> <span class="toc-text">Regularized Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-Algorithm"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">Gradient Descent Algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normal-Equation"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">Normal Equation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularized-Logistic-Regression"><span class="toc-number">2.2.3.</span> <span class="toc-text">Regularized Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-Algorithm-1"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">Gradient Descent Algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-for-Regularation-梯步迭代"><span class="toc-number">2.2.4.</span> <span class="toc-text">Summary for Regularation(梯步迭代)</span></a></li></ol></li></ol></li></ol>
		</div>
		
		<p> 机器学习课程笔记系列 I —— Andrew Ng w3</p>
<hr>
<h1 id="监督学习-classification问题"><a href="#监督学习-classification问题" class="headerlink" title="监督学习-classification问题"></a>监督学习-classification问题</h1><p>##Example</p>
<blockquote>
<p>Email:Spame / not spam?</p>
<p>Online Transactions:Fraudulent (yes/no)?</p>
<p>Tumor: Malignant/Benign?</p>
</blockquote>
<ul>
<li><p>二分类binary classification y:{0,1}.通常0代表negative class，1代表positive class</p>
</li>
<li><p>多分类multiple classification y： {0，1，2，3}</p>
</li>
</ul>
<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8A%E5%8D%889.01.12.png" alt="image"></p>
<p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8A%E5%8D%889.43.44.png" alt="image"><br>对于classification问题，如果采用linear regression取阀值的话，可能存在两个问题：</p>
<ol>
<li>预测结果不理想。由于通过训练集确定theta，可能训练集增加新的example（比如图最右边的红色x点）导致回归线的从紫色移动到蓝色。临界点从紫色实点移动到蓝色实点。</li>
<li>h(x)结果超出y的范围。e.g.可能小于0，或者大于1.</li>
</ol>
<p>因此，需要寻找一个新的模型——logistic regression，h(x)为单调递增连续函数，h(x)取值范围[0,1]</p>
<h2 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8A%E5%8D%889.13.21.png" alt="image"></p>
<p>h(x)=g(z)=1/(1+e^-z),z=theta*x</p>
<ol>
<li><p>h(x)含义：<br>对于input x，预测y=1的概率 P(y=1|x,theta)<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8A%E5%8D%889.33.57.png" alt="image"></p>
</li>
<li><p>h(x)怎么用于判断</p>
</li>
</ol>
<p>g(z)=h(x)&gt;0.5,或者z=theta*x&gt;0,预测y=1</p>
<p>g(z)=h(x)&lt;0.5,或者z=theta*x&lt;0,预测y=1<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8A%E5%8D%889.48.51.png" alt="image"></p>
<ol>
<li>h(x)使用案例：<br>反向，如果计算出了theta，通过z=-3+x1+x2&gt;0,预测y=1，decision boundary：x1+x2&gt;3 &lt;=&gt; h（x）&gt;0.5</li>
</ol>
<p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8A%E5%8D%889.55.07.png" alt="image"></p>
<p>decision boundary可能是非线性的：<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8B%E5%8D%8810.01.27.png" alt="image"></p>
<h3 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h3><h4 id="common-cost-function"><a href="#common-cost-function" class="headerlink" title="common cost function"></a>common cost function</h4><p>机器学习或者统计机器学习常见的损失函数如下：</p>
<ol>
<li>0-1损失函数 （0-1 loss function） </li>
</ol>
<p>L(Y,f(X))={1,0,Y ≠ f(X)Y = f(X)</p>
<ol>
<li>平方损失函数（quadratic loss function) </li>
</ol>
<p>L(Y,f(X))=(Y−f(x))^2</p>
<ol>
<li>绝对值损失函数(absolute loss function) </li>
</ol>
<p>L(Y,f(x))=|Y−f(X)|</p>
<ol>
<li>对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function) </li>
</ol>
<p>L(Y,P(Y|X))=−logP(Y|X)</p>
<ul>
<li><p>linear regression中，采用平方损失函数。</p>
</li>
<li><p>逻辑回归中，采用的则是对数损失函数。如果损失函数越小，表示模型越好。 </p>
<h4 id="cost-function-for-logistic-regression"><a href="#cost-function-for-logistic-regression" class="headerlink" title="cost function for logistic regression"></a>cost function for logistic regression</h4><p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8B%E5%8D%8811.10.28.png" alt="image"><br>由于y只会取0或者1，合并后<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8B%E5%8D%8811.13.24.png" alt="image"><br>cost function的解读：<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%886.34.37.png" alt="image"><br>如果y=1，对于h(x)-cost的曲线:如果h(x)-&gt;1,cost-&gt;0;如果h(x)-&gt;0,cost-&gt;无穷大的一个惩罚<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%886.28.33.png" alt="image"></p>
</li>
</ul>
<p>同理，对于y=1的h(x)-cost的曲线:如果h(x)-&gt;0,cost-&gt;0;如果h(x)-&gt;1,cost-&gt;无穷大<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%886.36.59.png" alt="image"></p>
<h4 id="证明cost-function"><a href="#证明cost-function" class="headerlink" title="证明cost function"></a>证明cost function</h4><p>由于</p>
<p>P(y=1|x,theta)=h(x)</p>
<p>P(y=0|x,theta)=1-h(x)</p>
<p>概率分布P(y|x,theta)=h(x)^y * (1-h(x))^(1-y)<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8B%E5%8D%88 11.38.13.png" alt="image"><br>由于每个样本是独立同分布，拟然函数如下：<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-11%20%E4%B8%8B%E5%8D%88 11.40.00.png" alt="image"><br>采用对数损失函数，<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%8812.00.53.png" alt="image"></p>
<h3 id="minimize-J"><a href="#minimize-J" class="headerlink" title="minimize J"></a>minimize J</h3><p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%8812.02.32.png" alt="image"></p>
<h4 id="1-Gradient-descent-algorithm"><a href="#1-Gradient-descent-algorithm" class="headerlink" title="1.Gradient descent algorithm"></a>1.Gradient descent algorithm</h4><p>对于J，线性回归与逻辑回归的cost function、h（x）不同。<br>但是，对于gradient descent（J对theta的导数）是一样的。只是其中h（x）的模型不同。<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%8812.03.47.png" alt="image"><br>蓝色的是线性回归的h（x），红色的是逻辑回归的<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%8812.08.20.png" alt="image"></p>
<blockquote>
<p>证明导数相似的过程</p>
</blockquote>
<ol>
<li>g(z)导数如下<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%8812.10.13.png" alt="image"></li>
<li>单个样本：<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%8812.17.22.png" alt="image"></li>
<li>扩展至全体样本<br><img src="/Users/wong/Desktop/屏幕快照 2016-07-12 上午12.24.00.png" alt="image"></li>
</ol>
<h4 id="2-other-alogrithms"><a href="#2-other-alogrithms" class="headerlink" title="2.other alogrithms"></a>2.other alogrithms</h4><p>Conjugate，BFGS，L-BFGS等，更聪明的迭代循环，不需要人工选择alpha，比GD更快，但是更复杂。<br><img src="/Users/wong/Desktop/屏幕快照 2016-07-12 上午12.27.21.png" alt="image"></p>
<h3 id="pseudocode"><a href="#pseudocode" class="headerlink" title="pseudocode"></a>pseudocode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">function [ jVal,gradient ] = costFunction( theta, X, y)  </div><div class="line">%hypothesis is logistic regression,costfunction returen J value and gradient     </div><div class="line">  jVal=(-y&apos;*log(sigmoid(X*theta))-(1-y)&apos;*log(1-sigmoid...(X*theta)))/m;</div><div class="line">  gradient=X&apos;*(sigmoid(X*theta)-y)/m; </div><div class="line"> </div><div class="line">end</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">针对</div><div class="line">options = optimset(&apos;GradObj&apos;,&apos;on&apos;,&apos;MaxIter&apos;,100);  </div><div class="line">initialTheta = zeros(2,1)  </div><div class="line">[optTheta,functionVal,exitFlag] = ... </div><div class="line">fminunc(@(t)(costFunction(t, X, y),initialTheta,options);</div></pre></td></tr></table></figure>
<h2 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h2><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.08.30.png" alt="image"><br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.09.03.png" alt="image"></p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p>n分类问题，转化为n个分类问题。分别通过训练数据训练出n个分类器，每个分类器可以区分i分类与其他。</p>
<p>一个新的x，分别放到n个分类器中计算，取可能性最大的一个<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.14.18.png" alt="image"><br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.17.08.png" alt="image"></p>
<h1 id="Solving-overfitting-problem"><a href="#Solving-overfitting-problem" class="headerlink" title="Solving overfitting problem"></a>Solving overfitting problem</h1><h2 id="Definition-overfitting"><a href="#Definition-overfitting" class="headerlink" title="Definition overfitting"></a>Definition overfitting</h2><p>对于之前讲的linear regression和logistic regression，都可能有overfitting的问题-准确率高，但是general的不好。</p>
<ul>
<li><p>对于linear regression<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%88 8.38.32.png" alt="image"></p>
</li>
<li><p>对于logistic regression<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.39.19.png" alt="image"></p>
</li>
</ul>
<p>通常，造成overfitting过拟合问题，都是feature太多。所以可以采取的主要办法有：</p>
<ol>
<li>减少特征值的个数（一种是人工选择保留那些特征值，还有一种是采用model selection algorithm）<strong><em> 但是可能去掉了有用的feature </em></strong></li>
<li>Regularation的方法（保留所有的feature，但是对每个feature增加一个惩罚项；）<strong><em> 尤其适用于feature多的情况。</em></strong><br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.45.02.png" alt="image"></li>
</ol>
<h2 id="Solution-Regularation"><a href="#Solution-Regularation" class="headerlink" title="Solution - Regularation"></a>Solution - Regularation</h2><h3 id="Model-cost-function"><a href="#Model-cost-function" class="headerlink" title="Model-cost function"></a>Model-cost function</h3><p>cost function中，增加一个lamba*theat^2的惩罚项。<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%888.51.31.png" alt="image"><br>如果lamda特别的大，使得theta-&gt;0，导致underfitting问题。<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.19.03.png" alt="image"></p>
<h3 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h3><h4 id="Gradient-Descent-Algorithm"><a href="#Gradient-Descent-Algorithm" class="headerlink" title="Gradient Descent Algorithm"></a>Gradient Descent Algorithm</h4><p><strong><em>惯例theta0没有惩罚项</em></strong><br>由于J中，总和中增加了lamba<em>thet1^2（从theta1开始），因此对J的导数中（Regularized J）增加了（1/m）\</em>lamda*theta-&gt;theta(除theta0)乘以一个小于1的系数，系数为(1-alpha*lamda/m)<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.22.33.png" alt="image"><br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.23.14.png" alt="image"></p>
<h4 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h4><p>regularized后，矩阵可逆,解决之前说的矩阵可能不存在逆矩阵的情况。？？？？<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.32.44.png" alt="image"><br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.31.54.png" alt="image"></p>
<h3 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h3><h4 id="Gradient-Descent-Algorithm-1"><a href="#Gradient-Descent-Algorithm-1" class="headerlink" title="Gradient Descent Algorithm"></a>Gradient Descent Algorithm</h4><p>同linear regression，theta0没有panelty。J的总和增加lamba*theat^2/2m.</p>
<p>gradient:theta(除theta0)乘以一个小于1的系数，系数为(1-alpha*lamda/m).<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.37.14.png" alt="image"><br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.39.58.png" alt="image"></p>
<p>###Pseudocode<br>J跟gradient中增加penalty,增加lambda。<br><img src="http://o9rmadjtd.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-12%20%E4%B8%8B%E5%8D%889.43.48.png" alt="image"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">function [ jVal,gradient ] = costFunction( theta, X, y,lambda)  </div><div class="line">%hypothesis is logistic regression,costfunction returen J value and gradient adding penalty  </div><div class="line">   n=size(theta);</div><div class="line">   </div><div class="line"></div><div class="line">   jVal=(-y&apos;*log(sigmoid(X*theta))-(1-y)&apos;*log(1-sigmoid(X*theta)))/m+lambda*theta(2:n,1)&apos;*theta(2:n,1)/(2*m);</div><div class="line"></div><div class="line">   gradiet=X&apos;*(sigmoid(X*theta)-y)/m+lambda*theta/m;</div><div class="line">   </div><div class="line"></div><div class="line">   temp=X&apos;*(sigmoid(X*theta)-y)/m;</div><div class="line">   gradient(1,1)=temp(1,1);</div><div class="line"> </div><div class="line">end</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">增加一个位置参数</div><div class="line">options = optimset(&apos;GradObj&apos;,&apos;on&apos;,&apos;MaxIter&apos;,100);  </div><div class="line">initialTheta = zeros(2,1)  </div><div class="line">[theta, J, exit_flag] = ...</div><div class="line">fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);</div></pre></td></tr></table></figure>
<h3 id="Summary-for-Regularation-梯步迭代"><a href="#Summary-for-Regularation-梯步迭代" class="headerlink" title="Summary for Regularation(梯步迭代)"></a>Summary for Regularation(梯步迭代)</h3><p>J(theta)=原J(theta)+lambda*theta(2:n,1)^2/(2<em>m)<br>gradient=原gradient+lambda\</em>theta/m<br>同步迭代时，theta=theta(1-alpha*lambda/m)-原gradient</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Coursera/">Coursera</a><a href="/tags/Andrew-Ng/">Andrew Ng</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
</div>



<div class="article-share" id="share">

  <div data-url="http://tnxy.github.io/2016/07/12/ML-w3/" data-title="ML-w3 | xiyuのworkspace" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 

<div class="next">
<a href="/2016/07/11/ML-w2/"  title="ML-w2">
 <strong>NEXT:</strong><br/> 
 <span>ML-w2
</span>
</a>
</div>

</nav>

	
<section class="comment">
	<div class="ds-thread"></div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#监督学习-classification问题"><span class="toc-number">1.</span> <span class="toc-text">监督学习-classification问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Preface"><span class="toc-number">1.1.</span> <span class="toc-text">Preface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-Model"><span class="toc-number">1.2.</span> <span class="toc-text">Logistic Regression Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition"><span class="toc-number">1.2.1.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cost-function"><span class="toc-number">1.2.2.</span> <span class="toc-text">cost function</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#common-cost-function"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">common cost function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cost-function-for-logistic-regression"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">cost function for logistic regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#证明cost-function"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">证明cost function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#minimize-J"><span class="toc-number">1.2.3.</span> <span class="toc-text">minimize J</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Gradient-descent-algorithm"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">1.Gradient descent algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-other-alogrithms"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">2.other alogrithms</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pseudocode"><span class="toc-number">1.2.4.</span> <span class="toc-text">pseudocode</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multiclass-Classification"><span class="toc-number">1.3.</span> <span class="toc-text">Multiclass Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Example"><span class="toc-number">1.3.1.</span> <span class="toc-text">Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solution"><span class="toc-number">1.3.2.</span> <span class="toc-text">Solution</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Solving-overfitting-problem"><span class="toc-number">2.</span> <span class="toc-text">Solving overfitting problem</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition-overfitting"><span class="toc-number">2.1.</span> <span class="toc-text">Definition overfitting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Solution-Regularation"><span class="toc-number">2.2.</span> <span class="toc-text">Solution - Regularation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-cost-function"><span class="toc-number">2.2.1.</span> <span class="toc-text">Model-cost function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularized-Linear-Regression"><span class="toc-number">2.2.2.</span> <span class="toc-text">Regularized Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-Algorithm"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">Gradient Descent Algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normal-Equation"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">Normal Equation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularized-Logistic-Regression"><span class="toc-number">2.2.3.</span> <span class="toc-text">Regularized Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-Algorithm-1"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">Gradient Descent Algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-for-Regularation-梯步迭代"><span class="toc-number">2.2.4.</span> <span class="toc-text">Summary for Regularation(梯步迭代)</span></a></li></ol></li></ol></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
			<li><a href="/categories/Machine-Learning/" title="Machine Learning">Machine Learning<sup>3</sup></a></li>
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			<li><a href="/tags/Andrew-Ng/" title="Andrew Ng">Andrew Ng<sup>3</sup></a></li>
		
			<li><a href="/tags/Coursera/" title="Coursera">Coursera<sup>3</sup></a></li>
		
			<li><a href="/tags/Machine-Learning/" title="Machine Learning">Machine Learning<sup>3</sup></a></li>
		
		</ul>
</div>


  <div class="rsspart">
	<a href="null" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font clearfix">
		
		
		
		<a href="https://github.com/https://github.com/tnxy" target="_blank" title="github"></a>
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2016 
		
		<a href="http://tnxy.github.io" target="_blank" title="xiyu">xiyu</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"null"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 





  </body>
</html>
